{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obfuscated Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Approach\n",
    "As this is an NLP problem, the classification can be done by\n",
    "1. Regular NeuralNetwork\n",
    "2. Recurrent Neural Network\n",
    "3. Convolutional Neural Network\n",
    "\n",
    "As the training set is obfuscated, the limitation of using RNN is I cannot use word embeddings and thus I can only build a character level model.\n",
    "\n",
    "First, a simple neural network is trained but the model performed poorly with an accuracy of less than 15%\n",
    "\n",
    "Then a convolution model is built based on paper by See Zhang and LeCun, 2015 for character level convolutional neural networks.\n",
    "\n",
    "I slightly fine tuned the model by modifying the convolutional layers outputs to 512 instead of 256 and 1024 as described in the paper.\n",
    "\n",
    "The model was performing well for training set but not so well for validation set as the data has huge imbalance\n",
    "\n",
    "## Handling data imbalance\n",
    "\n",
    "To balance the dataset, oversampling of the classes which has less number of samples is donbe by duplicating the examples.\n",
    "\n",
    "Rather than removing the samples for classes which have excess, Oversampling is done. This is because the training set is limited and I don't want to loose more information.\n",
    "\n",
    "*Another approach I tried to employ is building LSTM for each class and then using generative models to oversample the data instead of duplicating. *\n",
    "*As LSTM was character based, it was taking longer time, I didn't have resources to train such a network and hence didn't use that technique. (For training a single class, for one epoch LSTM ETA was almost 10 hours.)*\n",
    "\n",
    "\n",
    "The submission also contains the weight file 'weight_cnn_oversample_4.h5' which can be used for the model \n",
    "\n",
    "char_cnn2(n_vocab, max_len, n_classes, weights_path=None)\n",
    "\n",
    "The predictions for test data are in 'ytest.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Lambda, Bidirectional, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,ZeroPadding2D,Input,Activation\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import Embedding,ThresholdedReLU\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import re\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Reproducing consistent results\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading train data from txt\n",
    "train_df = pd.read_table('xtrain_obfuscated.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading train labels from txt\n",
    "train_labels = pd.read_table('ytrain.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[1] = train_labels\n",
    "train_df.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(xtrain, ytrain, xtest, max_len=None):\n",
    "    \"\"\"\n",
    "    Preprocess and featurize the data\n",
    "    \"\"\"\n",
    "\n",
    "    xtrain = [line.lower() for line in xtrain]\n",
    "    xtest = [line.lower() for line in xtest]\n",
    "    ytrain = [int(line) for line in ytrain]\n",
    "    \n",
    "    def chars(dataset):\n",
    "        return reduce(\n",
    "            lambda x, y: x.union(y),\n",
    "            (set(line) for line in dataset))\n",
    "    \n",
    "    def onehot(dataset, max_len, vocab_size):\n",
    "        hot = np.zeros((len(dataset), max_len, vocab_size), dtype=np.bool)\n",
    "        i = 0\n",
    "        for line in dataset:\n",
    "            j = 0\n",
    "            for char in line:\n",
    "                if char != 0:\n",
    "                    hot[i, j, char] = 1.\n",
    "\n",
    "                j += 1\n",
    "            i += 1\n",
    "\n",
    "        return hot\n",
    "  \n",
    "    # get all chars used in train as well as test\n",
    "    letters = chars(xtrain).union(chars(xtest))\n",
    "\n",
    "    # determine the maximum text length. in this regime, we are not truncating\n",
    "    # texts at all. in the paper texts are truncated.\n",
    "    max_text_length = np.max([np.max(list(map(len, ls))) for ls in [xtrain, xtest]])\n",
    "    max_len = max_len or max_text_length\n",
    "\n",
    "    # distinct letters and classes in the dataaset\n",
    "    vocab = sorted(list(letters))\n",
    "    classes = sorted(list(set(ytrain)))\n",
    "    # lookup tables for letters and classes. prepends padding char\n",
    "    idx_letters = dict(((c, i) for c, i in zip(vocab, range(len(vocab)))))\n",
    "    idx_classes = dict(((c, i) for c, i in zip(classes, range(len(classes)))))\n",
    "\n",
    "    # dense integral indices\n",
    "    xtrain = [[idx_letters[char] for char in list(line)] for line in xtrain]\n",
    "    xtest = [[idx_letters[char] for char in list(line)] for line in xtest]\n",
    "    ytrain = [idx_classes[line] for line in ytrain]\n",
    "\n",
    "    # pad to fixed lengths\n",
    "    xtrain = sequence.pad_sequences(xtrain, max_len)\n",
    "    xtest = sequence.pad_sequences(xtest, max_len)\n",
    "\n",
    "    xtrain = onehot(xtrain, max_len, len(idx_letters))\n",
    "    ytrain = to_categorical(ytrain, nb_classes=len(classes))\n",
    "    xtest = onehot(xtest, max_len, len(idx_letters))\n",
    "\n",
    "    return (\n",
    "        xtrain,\n",
    "        ytrain,\n",
    "        xtest,\n",
    "        vocab,\n",
    "        max_len,\n",
    "        len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lines(filename):\n",
    "        with open(filename) as f:\n",
    "            return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(model, xtrain, ytrain, batch=128, epochs=5, split=0.1,class_weights=None,validation_data=None):\n",
    "    \"fit the model\"\n",
    "\n",
    "    return model.fit(xtrain,\n",
    "                     ytrain,\n",
    "                     batch_size=batch,\n",
    "                     nb_epoch=epochs,\n",
    "                     validation_split=split,\n",
    "                    class_weight = class_weights,\n",
    "                    validation_data = validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compiled(model):\n",
    "    \"compile with chosen config\"\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \"predict probability, class for each instance\"\n",
    "\n",
    "    # predict probability of each class for each instance\n",
    "    all_preds = model.predict(X)\n",
    "\n",
    "    # for each instance get the index of the class with max probability\n",
    "    idxs = np.argmax(all_preds, axis=1)\n",
    "\n",
    "    # get the values of the highest probability for each instance\n",
    "    preds = [all_preds[i, idxs[i]] for i in range(len(idxs))]\n",
    "\n",
    "    return np.array(preds), idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to set class weights for imbalanced data\n",
    "def set_class_weights(labels):\n",
    "    class_counts = Counter(labels)\n",
    "    class_weights = {}\n",
    "    label_len = len(labels)\n",
    "    max_index = max(class_counts, key=class_counts.get)\n",
    "    for item,i in enumerate(class_counts):\n",
    "        class_weights[i] = round(class_counts[max_index]/class_counts[i])\n",
    "    max_index = max(class_weights, key=class_weights.get)\n",
    "    #Setting weights proportional to inversely proportional to max_value\n",
    "    #for item,i in enumerate(class_weights):\n",
    "        #class_weights[i] = class_weights[max_index]/(class_weights[i]+0.1) #Adding 0.1 to tackle if value 0\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resampling the input data based on class imbalance\n",
    "def build_resample0(labels,label_counts,df):\n",
    "    max_count = np.max(label_counts)\n",
    "    new_df = df.copy()\n",
    "    for label in labels:\n",
    "        label_df = df.loc[df['label']==label].copy(deep=True)\n",
    "        diff = max_count-label_counts.iloc[label]\n",
    "        to_add = float(diff)/float(label_counts.loc[label])\n",
    "        fraction_samples = to_add if to_add<1 else to_add-round(to_add)\n",
    "        for i in range(0,int(round(to_add))):\n",
    "            new_df = new_df.append(label_df)\n",
    "        if fraction_samples >0:\n",
    "            samples = int(round(label_df.shape[0]*fraction_samples))\n",
    "            frac_df = label_df.iloc[:samples,:]\n",
    "            new_df = new_df.append(frac_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "Trying a simple neural network and checking if how the model preforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(300, input_dim=452, init=\"uniform\",activation=\"relu\"))\n",
    "simple_model.add(Dense(200, init=\"uniform\", activation=\"relu\"))\n",
    "simple_model.add(Dense(12))\n",
    "simple_model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.8, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_model.compile(sgd, 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_chars(df):\n",
    "    charset={''}\n",
    "    for item in df:\n",
    "        charset.add(''.join(set(item)))\n",
    "    charset = set(''.join(charset))\n",
    "    return charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_numeric(text,char_set):\n",
    "    numeric = []\n",
    "    i=0\n",
    "    for char in text:\n",
    "        numeric.append(str(char_set.index(char)+1))\n",
    "    return numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_df['text']\n",
    "train_labels = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "charset = list(get_unique_chars(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_numeric = []\n",
    "for item in train_features:\n",
    "    train_numeric.append(text_to_numeric(item,charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padded_train_features = sequence.pad_sequences(train_numeric, maxlen=452, dtype='int32',\n",
    "    padding='post', truncating='post', value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32513, 452)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/4\n",
      "26010/26010 [==============================] - 1s - loss: 13.5616 - acc: 0.1577 - val_loss: 13.6594 - val_acc: 0.1525\n",
      "Epoch 2/4\n",
      "26010/26010 [==============================] - 1s - loss: 13.5743 - acc: 0.1578 - val_loss: 13.6594 - val_acc: 0.1525\n",
      "Epoch 3/4\n",
      "26010/26010 [==============================] - 1s - loss: 13.5743 - acc: 0.1578 - val_loss: 13.6594 - val_acc: 0.1525\n",
      "Epoch 4/4\n",
      "26010/26010 [==============================] - 1s - loss: 13.5743 - acc: 0.1578 - val_loss: 13.6594 - val_acc: 0.1525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47c4c7ded0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.fit(padded_train_features, keras.utils.np_utils.to_categorical(train_labels), nb_epoch=4, batch_size=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model is Poor\n",
    "Simple model performs poorly. Eventhogh there is less overfitting, the accuracy levels are very less and not improving in every epoch.\n",
    "Try to define a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(10000, input_dim=452, init=\"uniform\",activation=\"relu\"))\n",
    "simple_model.add(Dense(8000,activation=\"relu\"))\n",
    "simple_model.add(Dense(6000, activation=\"relu\"))\n",
    "simple_model.add(Dense(4000,activation=\"relu\"))\n",
    "simple_model.add(Dense(2000,activation=\"relu\"))\n",
    "simple_model.add(Dense(1000,activation=\"relu\"))\n",
    "simple_model.add(Dense(500,activation=\"relu\"))\n",
    "simple_model.add(Dense(12))\n",
    "simple_model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_model.compile(sgd, 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/2\n",
      "26010/26010 [==============================] - 59s - loss: 14.2737 - acc: 0.1131 - val_loss: 14.4128 - val_acc: 0.1058\n",
      "Epoch 2/2\n",
      "26010/26010 [==============================] - 59s - loss: 14.2925 - acc: 0.1133 - val_loss: 14.4128 - val_acc: 0.1058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47c3a78d10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.fit(padded_train_features, keras.utils.np_utils.to_categorical(train_labels), nb_epoch=2, batch_size=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still Poor\n",
    "Incresing model complexity doesn't increse the performance\n",
    "\n",
    "Trying to onehot encode train data and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11752, 32513)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsimpletrain,ysimpletrain,testsimpletrain,vocab, max_len, n_classes = prepare_data(\n",
    "        lines('xtrain_obfuscated.txt'),\n",
    "        lines('ytrain.txt'),\n",
    "        lines('xtest_obfuscated.txt'))\n",
    "train_x_flatten = xsimpletrain.reshape(xsimpletrain.shape[0], -1) \n",
    "train_x_flatten.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(10000, input_dim=train_x_flatten.shape[1], activation=\"relu\"))\n",
    "simple_model.add(Dense(8000,activation=\"relu\"))\n",
    "simple_model.add(Dense(6000, activation=\"relu\"))\n",
    "simple_model.add(Dense(4000,activation=\"relu\"))\n",
    "simple_model.add(Dense(2000,activation=\"relu\"))\n",
    "simple_model.add(Dense(1000,activation=\"relu\"))\n",
    "simple_model.add(Dense(500,activation=\"relu\"))\n",
    "simple_model.add(Dense(12))\n",
    "simple_model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_model.compile(sgd, 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/2\n",
      "26010/26010 [==============================] - 92s - loss: 15.2879 - acc: 0.0454 - val_loss: 15.3844 - val_acc: 0.0455\n",
      "Epoch 2/2\n",
      "26010/26010 [==============================] - 92s - loss: 15.3900 - acc: 0.0452 - val_loss: 15.3844 - val_acc: 0.0455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47c2985b90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.fit(train_x_flatten, keras.utils.np_utils.to_categorical(train_labels), nb_epoch=2, batch_size=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model still doesn't improve for each epoch. Trying different architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the model architecture\n",
    "\n",
    "As the train data is obfuscated, character level models can be fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model is inspired from paper \"See Zhang and LeCun, 2015\" for character level CNN\n",
    "def char_cnn1(n_vocab, max_len, n_classes, weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(256, 7, activation='relu', input_shape=(max_len, n_vocab)))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(256, 7, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(Conv1D(256, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xcnn1,ycnn1,testcnn1,vocab, max_len, n_classes = prepare_data(\n",
    "        lines('xtrain_obfuscated.txt'),\n",
    "        lines('ytrain.txt'),\n",
    "        lines('xtest_obfuscated.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1_cnn1 = compiled(char_cnn1(len(vocab), max_len, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29261 samples, validate on 3252 samples\n",
      "Epoch 1/20\n",
      "29261/29261 [==============================] - 43s - loss: 2.3387 - acc: 0.1581 - val_loss: 2.1734 - val_acc: 0.2263\n",
      "Epoch 2/20\n",
      "29261/29261 [==============================] - 43s - loss: 1.8809 - acc: 0.3429 - val_loss: 1.6528 - val_acc: 0.4197\n",
      "Epoch 3/20\n",
      "29261/29261 [==============================] - 43s - loss: 1.5181 - acc: 0.4660 - val_loss: 1.4502 - val_acc: 0.4938\n",
      "Epoch 4/20\n",
      "29261/29261 [==============================] - 43s - loss: 1.3515 - acc: 0.5234 - val_loss: 1.3849 - val_acc: 0.5194\n",
      "Epoch 5/20\n",
      "29261/29261 [==============================] - 43s - loss: 1.2145 - acc: 0.5788 - val_loss: 1.3997 - val_acc: 0.5234\n",
      "Epoch 6/20\n",
      "29261/29261 [==============================] - 43s - loss: 1.1052 - acc: 0.6128 - val_loss: 1.1779 - val_acc: 0.5867\n",
      "Epoch 7/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.9718 - acc: 0.6594 - val_loss: 1.1485 - val_acc: 0.6052\n",
      "Epoch 8/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.8890 - acc: 0.6866 - val_loss: 1.1314 - val_acc: 0.6199\n",
      "Epoch 9/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.7916 - acc: 0.7234 - val_loss: 1.1029 - val_acc: 0.6587\n",
      "Epoch 10/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.7091 - acc: 0.7536 - val_loss: 1.1494 - val_acc: 0.6568\n",
      "Epoch 11/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.6421 - acc: 0.7786 - val_loss: 1.1547 - val_acc: 0.6559\n",
      "Epoch 12/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.5874 - acc: 0.7967 - val_loss: 1.1404 - val_acc: 0.6823\n",
      "Epoch 13/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.5270 - acc: 0.8187 - val_loss: 1.1122 - val_acc: 0.6796\n",
      "Epoch 14/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.4826 - acc: 0.8336 - val_loss: 1.2607 - val_acc: 0.6676\n",
      "Epoch 15/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.4302 - acc: 0.8535 - val_loss: 1.2841 - val_acc: 0.6756\n",
      "Epoch 16/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.3914 - acc: 0.8663 - val_loss: 1.2633 - val_acc: 0.6851\n",
      "Epoch 17/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.3550 - acc: 0.8809 - val_loss: 1.3149 - val_acc: 0.6771\n",
      "Epoch 18/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.3191 - acc: 0.8910 - val_loss: 1.5165 - val_acc: 0.6593\n",
      "Epoch 19/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.2892 - acc: 0.9023 - val_loss: 1.4988 - val_acc: 0.6722\n",
      "Epoch 20/20\n",
      "29261/29261 [==============================] - 43s - loss: 0.2819 - acc: 0.9049 - val_loss: 1.4322 - val_acc: 0.6820\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit1 = fit(model1_cnn1, xcnn1, ycnn1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1_cnn1.save_weights('weights_cnn1_fit1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN1 Intuition\n",
    "\n",
    "As the model is ran for 20 epochs, the good thing is the train accuracy of the model is increasing after each epoch.\n",
    "\n",
    "However, theere is a gap between train and validation accuracy. The nmodel seem to be overfitting.\n",
    "\n",
    "Inorder to reduce overfitting, first chack if the dataset has imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imbalance = Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 543,\n",
       "         1: 3459,\n",
       "         2: 1471,\n",
       "         3: 4023,\n",
       "         4: 2337,\n",
       "         5: 2283,\n",
       "         6: 4226,\n",
       "         7: 5097,\n",
       "         8: 3634,\n",
       "         9: 980,\n",
       "         10: 3052,\n",
       "         11: 1408})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that, the dataset is highly skewed. \n",
    "\n",
    "Inorder to tackle this, set class weights proportional to the level of imbalance.\n",
    "For example: If label 7 is 15% of dataset, and label 1 is 2%, then setting weight of 1 to label 7 and 13 to label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = set_class_weights(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 9.0,\n",
       " 1: 1.0,\n",
       " 2: 3.0,\n",
       " 3: 1.0,\n",
       " 4: 2.0,\n",
       " 5: 2.0,\n",
       " 6: 1.0,\n",
       " 7: 1.0,\n",
       " 8: 1.0,\n",
       " 9: 5.0,\n",
       " 10: 1.0,\n",
       " 11: 3.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29261 samples, validate on 3252 samples\n",
      "Epoch 1/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.9080 - acc: 0.8235 - val_loss: 1.9711 - val_acc: 0.6602\n",
      "Epoch 2/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.7818 - acc: 0.8431 - val_loss: 2.1575 - val_acc: 0.6611\n",
      "Epoch 3/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.6239 - acc: 0.8678 - val_loss: 2.1187 - val_acc: 0.6876\n",
      "Epoch 4/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.4935 - acc: 0.8919 - val_loss: 2.1023 - val_acc: 0.6737\n",
      "Epoch 5/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.4370 - acc: 0.8998 - val_loss: 2.2353 - val_acc: 0.6879\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit1 = fit(model1_cnn1, xcnn1, ycnn1, epochs=5, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29261 samples, validate on 3252 samples\n",
      "Epoch 1/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.3826 - acc: 0.9137 - val_loss: 2.4520 - val_acc: 0.6863\n",
      "Epoch 2/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.3210 - acc: 0.9272 - val_loss: 2.8318 - val_acc: 0.6811\n",
      "Epoch 3/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.3454 - acc: 0.9216 - val_loss: 2.3589 - val_acc: 0.6753\n",
      "Epoch 4/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.2749 - acc: 0.9347 - val_loss: 2.5576 - val_acc: 0.6851\n",
      "Epoch 5/5\n",
      "29261/29261 [==============================] - 43s - loss: 0.2534 - acc: 0.9392 - val_loss: 2.9472 - val_acc: 0.6867\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit1 = fit(model1_cnn1, xcnn1, ycnn1, epochs=5, class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition2\n",
    "\n",
    "Balancing the class weights doesn't effect the model performance. There is no improvement from the previous model.\n",
    "\n",
    "So, now the train data need to be balanced by under sampling or oversampling.\n",
    "\n",
    "Undersampling can be reducing the number of examples of certain labels and making the training set balanced. The problem with this approach is, we will end up loosing lot of information for training and might lead to increrase in bias.\n",
    "\n",
    "Oversampling is reverse process to undersampling. Whereas here, we duplicate examples for labels which are less in number so that the training set is balanced. \n",
    "\n",
    "One important point while oversampling is, seperate the validation set before doing oversmpling so that the validation set doesn't have duplicates and hence will give the exact performance of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffling the data once again to make sure data is not arranged in any order\n",
    "train_df = train_df.iloc[np.random.permutation(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting train and val samples\n",
    "val_sample_cnn1 = train_df.iloc[:int(round(train_df.shape[0]*0.1)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_sample_cnn1 = train_df.iloc[int(round(train_df.shape[0]*0.1)):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_sample_cnn1.columns= ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_counts = trn_sample_cnn1.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_unique_labels = trn_sample_cnn1.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4619"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_unique_labels\n",
    "label_counts.loc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Balancing the sample data by duplicating data for labels which are less in number\n",
    "resampled_trn_cnn1 = build_resample(sample_unique_labels,label_counts,trn_sample_cnn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_resample(labels,label_counts,df):\n",
    "    max_count = np.max(label_counts)\n",
    "    new_df = df.copy(deep=True)\n",
    "    samples=0\n",
    "    for label in labels:\n",
    "        label_df = df.loc[df['label']==label]\n",
    "        diff = max_count-label_counts.loc[label]\n",
    "        to_add = float(diff)/float(label_counts.loc[label])\n",
    "        fraction_samples = to_add if to_add<1 else to_add-round(to_add)\n",
    "        for i in range(0,int(round(to_add))):\n",
    "            adding = int(round(to_add))\n",
    "            new_df = new_df.append(label_df)\n",
    "        if fraction_samples >0:\n",
    "            samples = int(round(label_df.shape[0]*fraction_samples))\n",
    "            frac_df = label_df.iloc[:samples,:].copy(deep=True)\n",
    "            new_df = new_df.append(frac_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resampled_trn_cnn1 = resampled_trn_cnn1.iloc[np.random.permutation(len(resampled_trn_cnn1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xcnn1_sampled, ycnn1_sampled, testcnn1_sampled, vocab, max_len, n_classes = prepare_data(\n",
    "        resampled_trn_cnn1['text'],\n",
    "        resampled_trn_cnn1['label'],\n",
    "        lines('xtest_obfuscated.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_features, val_label, testcnn1_sampled1, vocab, max_len, n_classes = prepare_data(\n",
    "        val_sample_cnn1['text'],\n",
    "        val_sample_cnn1['label'],\n",
    "        lines('xtest_obfuscated.txt'),\n",
    "        max_len=452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Further changing few things in model. (Changing outputs to 512 instead of 256)\n",
    "def char_cnn2(n_vocab, max_len, n_classes, weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(512, 7, activation='relu', input_shape=(max_len, n_vocab)))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(512, 7, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_cnn1 = compiled(char_cnn2(len(vocab), max_len, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53381 samples, validate on 5932 samples\n",
      "Epoch 1/5\n",
      "53381/53381 [==============================] - 214s - loss: 2.4436 - acc: 0.1341 - val_loss: 2.3733 - val_acc: 0.1714\n",
      "Epoch 2/5\n",
      "53381/53381 [==============================] - 214s - loss: 1.9516 - acc: 0.2785 - val_loss: 1.8103 - val_acc: 0.3132\n",
      "Epoch 3/5\n",
      "53381/53381 [==============================] - 394s - loss: 1.5977 - acc: 0.4019 - val_loss: 1.4575 - val_acc: 0.4749\n",
      "Epoch 4/5\n",
      "53381/53381 [==============================] - 214s - loss: 1.3025 - acc: 0.5236 - val_loss: 1.1302 - val_acc: 0.5905\n",
      "Epoch 5/5\n",
      "53381/53381 [==============================] - 214s - loss: 0.9817 - acc: 0.6463 - val_loss: 0.8953 - val_acc: 0.6918\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit3 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53381 samples, validate on 5932 samples\n",
      "Epoch 1/5\n",
      "53381/53381 [==============================] - 213s - loss: 0.6975 - acc: 0.7531 - val_loss: 0.7283 - val_acc: 0.7478\n",
      "Epoch 2/5\n",
      "53381/53381 [==============================] - 213s - loss: 0.5406 - acc: 0.8108 - val_loss: 0.5518 - val_acc: 0.8205\n",
      "Epoch 3/5\n",
      "53381/53381 [==============================] - 213s - loss: 0.4204 - acc: 0.8513 - val_loss: 0.5455 - val_acc: 0.8269\n",
      "Epoch 4/5\n",
      "53381/53381 [==============================] - 213s - loss: 0.3686 - acc: 0.8727 - val_loss: 0.4541 - val_acc: 0.8549\n",
      "Epoch 5/5\n",
      "53381/53381 [==============================] - 213s - loss: 0.3056 - acc: 0.8936 - val_loss: 0.4919 - val_acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit4 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53381 samples, validate on 5932 samples\n",
      "Epoch 1/5\n",
      "53381/53381 [==============================] - 215s - loss: 0.2593 - acc: 0.9118 - val_loss: 0.4340 - val_acc: 0.8712\n",
      "Epoch 2/5\n",
      "53381/53381 [==============================] - 239s - loss: 0.2165 - acc: 0.9274 - val_loss: 0.5347 - val_acc: 0.8424\n",
      "Epoch 3/5\n",
      "53381/53381 [==============================] - 242s - loss: 0.2085 - acc: 0.9319 - val_loss: 0.4204 - val_acc: 0.8796\n",
      "Epoch 4/5\n",
      "53381/53381 [==============================] - 238s - loss: 0.1874 - acc: 0.9390 - val_loss: 0.4015 - val_acc: 0.8953\n",
      "Epoch 5/5\n",
      "53381/53381 [==============================] - 221s - loss: 0.1566 - acc: 0.9487 - val_loss: 0.3750 - val_acc: 0.8992\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit5 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_cnn1.save_weights('weight_cnn_oversample_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "Now the model seem to be less overfitting.\n",
    "\n",
    "As the validation data is set aside, now training on the complete dataset and feeding the validation set in validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59313 samples, validate on 3251 samples\n",
      "Epoch 1/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.1615 - acc: 0.9495 - val_loss: 1.5835 - val_acc: 0.6921\n",
      "Epoch 2/5\n",
      "59313/59313 [==============================] - 246s - loss: 0.1338 - acc: 0.9572 - val_loss: 1.6058 - val_acc: 0.6801\n",
      "Epoch 3/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.1296 - acc: 0.9595 - val_loss: 1.6524 - val_acc: 0.6949\n",
      "Epoch 4/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.1216 - acc: 0.9617 - val_loss: 1.6030 - val_acc: 0.6835\n",
      "Epoch 5/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.1252 - acc: 0.9608 - val_loss: 1.5418 - val_acc: 0.6887\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit5 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5,split=0,validation_data=(val_features,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_cnn1.save_weights('weight_cnn_oversample_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59313 samples, validate on 3251 samples\n",
      "Epoch 1/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.0913 - acc: 0.9712 - val_loss: 1.6428 - val_acc: 0.6856\n",
      "Epoch 2/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.0994 - acc: 0.9697 - val_loss: 1.7381 - val_acc: 0.6823\n",
      "Epoch 3/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.1051 - acc: 0.9688 - val_loss: 1.7726 - val_acc: 0.6921\n",
      "Epoch 4/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.0990 - acc: 0.9713 - val_loss: 1.6515 - val_acc: 0.7032\n",
      "Epoch 5/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.0955 - acc: 0.9720 - val_loss: 1.7677 - val_acc: 0.7056\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit6 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5,split=0,validation_data=(val_features,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_cnn1.save_weights('weight_cnn_oversample_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59313 samples, validate on 3251 samples\n",
      "Epoch 1/5\n",
      "59313/59313 [==============================] - 231s - loss: 0.0812 - acc: 0.9763 - val_loss: 1.7998 - val_acc: 0.6961\n",
      "Epoch 2/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.0913 - acc: 0.9740 - val_loss: 1.6959 - val_acc: 0.6884\n",
      "Epoch 3/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.0704 - acc: 0.9789 - val_loss: 1.6921 - val_acc: 0.7093\n",
      "Epoch 4/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.1040 - acc: 0.9699 - val_loss: 1.6697 - val_acc: 0.6992\n",
      "Epoch 5/5\n",
      "59313/59313 [==============================] - 232s - loss: 0.0798 - acc: 0.9775 - val_loss: 1.6773 - val_acc: 0.7013\n"
     ]
    }
   ],
   "source": [
    "char_cnn1_fit6 = fit(model2_cnn1, xcnn1_sampled, ycnn1_sampled, epochs=5,split=0,validation_data=(val_features,val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_cnn1.save_weights('weight_cnn_oversample_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '2453' (I am process '2165')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-2.7.12-64/lock_dir\n"
     ]
    }
   ],
   "source": [
    "preds,idx = predict(model2_cnn1,testcnn1_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytext_file = open('ytext.txt','w') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in idx:\n",
    "    ytext_file.write(str(i)+'\\n')\n",
    "ytext_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "As the train accuracy seems good, validation acccuracy is less and the model seem to be still overfitting.\n",
    "\n",
    "### From Above cells it is clear that labels with 4,5,2,11,9,0 have have very few examples.\n",
    "So, building an LTSM model for them and generate the text later using generative model to oversample the data instead of duplicating the data as it was done earlier\n",
    "\n",
    "For other labels, data is balanced. So for these labels, I will oversample using duplication method as done previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "As the validation accuracy doesn't increase using CNN, trying LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_ltsm_data(raw_text):\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(raw_text)\n",
    "    n_vocab = len(chars)\n",
    "    print \"Total Characters: \", n_chars\n",
    "    print \"Total Vocab: \", n_vocab\n",
    "    # prepare the dataset of input to output pairs encoded as integers\n",
    "    seq_length = 452\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "    X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    # normalize\n",
    "    X = X / float(n_vocab)\n",
    "    # one hot encode the output variable\n",
    "    y = to_categorical(dataY)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(452, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model,label,X,y,epochs,batch_size=128,verbose=1):\n",
    "    print('label: '+str(label))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    filepath=\"weights-ltsm\"+str(label)+\"-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    model.fit(X,\n",
    "              y,\n",
    "              nb_epoch=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=callbacks_list,\n",
    "              verbose=verbose)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
