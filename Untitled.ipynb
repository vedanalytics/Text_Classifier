{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Lambda, Bidirectional, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import re\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(x, sz=71):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def striphtml(s):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_table('xtrain_obfuscated.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_table('ytrain.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_seq_len = train_features.apply(lambda(x):len(x)).max()\n",
    "max_seq_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unique_chars(df):\n",
    "    charset={''}\n",
    "    for item in df:\n",
    "        charset.add(''.join(set(item)))\n",
    "    charset = set(''.join(charset))\n",
    "    return charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "charset = list(get_unique_chars(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_numeric(text,char_set):\n",
    "    numeric = []\n",
    "    i=0\n",
    "    for char in text:\n",
    "        numeric.append(str(char_set.index(char)+1))\n",
    "    return numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iu = 0\n",
    "train_numeric = []\n",
    "for item in train_features:\n",
    "    train_numeric.append(text_to_numeric(item,charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padded_train_features = sequence.pad_sequences(train_numeric, maxlen=max_seq_len, dtype='int32',\n",
    "    padding='post', truncating='post', value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "TEXT_LENGTH = 500\n",
    "NB_FILTER = [64, 128]\n",
    "NB_GRAM = [4, 3, 3]\n",
    "FULLY_CONNECTED_UNIT = 256\n",
    "DROPOUT = [0.7, 0.7]  # dropout rate in 2 fully connected layers.\n",
    "\n",
    "# Training parameters\n",
    "TEST_PERCENT = 0.1  # test data %\n",
    "VALID_PERCENT = 0.1  # valid data %\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "EARLY_STOP = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,ZeroPadding2D,Input,Activation\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, Embedding,ThresholdedReLU\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvBlock(layers, model, filters):\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FCBlock(model):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HappyModel(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_dim=500, init=\"uniform\",activation=\"relu\"))\n",
    "    model.add(Dense(200, init=\"uniform\", activation=\"relu\"))\n",
    "    model.add(Dense(12))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happyModel = HappyModel(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.8, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "happyModel.compile(sgd, 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/4\n",
      "26010/26010 [==============================] - 1s - loss: 14.1351 - acc: 0.1230 - val_loss: 14.0782 - val_acc: 0.1266\n",
      "Epoch 2/4\n",
      "26010/26010 [==============================] - 1s - loss: 14.1351 - acc: 0.1230 - val_loss: 14.0782 - val_acc: 0.1266\n",
      "Epoch 3/4\n",
      "26010/26010 [==============================] - 1s - loss: 14.1351 - acc: 0.1230 - val_loss: 14.0782 - val_acc: 0.1266\n",
      "Epoch 4/4\n",
      "26010/26010 [==============================] - 1s - loss: 14.1351 - acc: 0.1230 - val_loss: 14.0782 - val_acc: 0.1266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd5ee0e210>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happyModel.fit(padded_train_features, keras.utils.np_utils.to_categorical(train_labels), nb_epoch=4, batch_size=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char_cnn(n_vocab, max_len, n_classes, weights_path=None):\n",
    "    \"See Zhang and LeCun, 2015\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(512, 7, activation='relu', input_shape=(max_len, n_vocab)))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(512, 7, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(Conv1D(512, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compiled(model):\n",
    "    \"compile with chosen config\"\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(model, xtrain, ytrain, batch=128, epochs=5, split=0.1,class_weights=None):\n",
    "    \"fit the model\"\n",
    "\n",
    "    return model.fit(xtrain,\n",
    "                     ytrain,\n",
    "                     batch_size=batch,\n",
    "                     nb_epoch=epochs,\n",
    "                     validation_split=split,\n",
    "                    class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \"predict probability, class for each instance\"\n",
    "\n",
    "    # predict probability of each class for each instance\n",
    "    all_preds = model.predict(X)\n",
    "\n",
    "    # for each instance get the index of the class with max probability\n",
    "    idxs = np.argmax(all_preds, axis=1)\n",
    "\n",
    "    # get the values of the highest probability for each instance\n",
    "    preds = [all_preds[i, idxs[i]] for i in range(len(idxs))]\n",
    "\n",
    "    return np.array(preds), idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def lines(filename):\n",
    "        with open(filename) as f:\n",
    "            return f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(xtrain, ytrain, xtest, max_len=None):\n",
    "    \"\"\"\n",
    "    Preprocess and featurize the data\n",
    "    \"\"\"\n",
    "\n",
    "    xtrain = [line.lower() for line in xtrain]\n",
    "    xtest = [line.lower() for line in xtest]\n",
    "    ytrain = [int(line) for line in ytrain]\n",
    "\n",
    "    def chars(dataset):\n",
    "        return reduce(\n",
    "            lambda x, y: x.union(y),\n",
    "            (set(line) for line in dataset))\n",
    "\n",
    "    def onehot(dataset, max_len, vocab_size):\n",
    "        hot = np.zeros((len(dataset), max_len, vocab_size), dtype=np.bool)\n",
    "        i = 0\n",
    "        for line in dataset:\n",
    "            j = 0\n",
    "            for char in line:\n",
    "                if char != 0:\n",
    "                    hot[i, j, char] = 1.\n",
    "\n",
    "                j += 1\n",
    "            i += 1\n",
    "\n",
    "        return hot\n",
    "\n",
    "    # get all chars used in train as well as test\n",
    "    letters = chars(xtrain).union(chars(xtest))\n",
    "\n",
    "    # determine the maximum text length. in this regime, we are not truncating\n",
    "    # texts at all. in the paper texts are truncated.\n",
    "    max_text_length = np.max([np.max(list(map(len, ls))) for ls in [xtrain, xtest]])\n",
    "    max_len = max_len or max_text_length\n",
    "\n",
    "    # distinct letters and classes in the dataaset\n",
    "    vocab = ['ï¿½'] + sorted(list(letters))\n",
    "    classes = sorted(list(set(ytrain)))\n",
    "\n",
    "    # lookup tables for letters and classes. prepends padding char\n",
    "    idx_letters = dict(((c, i) for c, i in zip(vocab, range(len(vocab)))))\n",
    "    idx_classes = dict(((c, i) for c, i in zip(classes, range(len(classes)))))\n",
    "\n",
    "    # dense integral indices\n",
    "    xtrain = [[idx_letters[char] for char in list(line)] for line in xtrain]\n",
    "    xtest = [[idx_letters[char] for char in list(line)] for line in xtest]\n",
    "    ytrain = [idx_classes[line] for line in ytrain]\n",
    "\n",
    "    # pad to fixed lengths\n",
    "    xtrain = sequence.pad_sequences(xtrain, max_len)\n",
    "    xtest = sequence.pad_sequences(xtest, max_len)\n",
    "\n",
    "    xtrain = onehot(xtrain, max_len, len(idx_letters))\n",
    "    ytrain = to_categorical(ytrain, nb_classes=len(classes))\n",
    "    xtest = onehot(xtest, max_len, len(idx_letters))\n",
    "\n",
    "    return (\n",
    "        xtrain,\n",
    "        ytrain,\n",
    "        xtest,\n",
    "        vocab,\n",
    "        max_len,\n",
    "        len(classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_class_weights(labels):\n",
    "    class_counts = Counter(labels)\n",
    "    class_weights = {}\n",
    "    label_len = len(labels)\n",
    "    for item,i in enumerate(class_counts):\n",
    "        class_weights[i] = round((class_counts[i]*100)/label_len)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = set_class_weights(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 10.0,\n",
       " 2: 4.0,\n",
       " 3: 12.0,\n",
       " 4: 7.0,\n",
       " 5: 7.0,\n",
       " 6: 12.0,\n",
       " 7: 15.0,\n",
       " 8: 11.0,\n",
       " 9: 3.0,\n",
       " 10: 9.0,\n",
       " 11: 4.0}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain, ytrain, xtest, vocab, max_len, n_classes = preprocess(\n",
    "        lines('xtrain_obfuscated.txt'),\n",
    "        lines('ytrain.txt'),\n",
    "        lines('xtest_obfuscated.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = compiled(char_cnn(len(vocab), max_len, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('weights_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29261 samples, validate on 3252 samples\n",
      "Epoch 1/20\n",
      "29261/29261 [==============================] - 41s - loss: 20.6845 - acc: 0.1791 - val_loss: 18.8512 - val_acc: 0.2202\n",
      "Epoch 2/20\n",
      "29261/29261 [==============================] - 41s - loss: 16.9286 - acc: 0.2981 - val_loss: 15.4067 - val_acc: 0.3220\n",
      "Epoch 3/20\n",
      "29261/29261 [==============================] - 41s - loss: 14.4466 - acc: 0.3971 - val_loss: 12.5718 - val_acc: 0.4656\n",
      "Epoch 4/20\n",
      "29261/29261 [==============================] - 41s - loss: 11.4266 - acc: 0.5218 - val_loss: 11.8109 - val_acc: 0.5203\n",
      "Epoch 5/20\n",
      "29261/29261 [==============================] - 41s - loss: 9.8215 - acc: 0.5815 - val_loss: 10.1113 - val_acc: 0.5673\n",
      "Epoch 6/20\n",
      "29261/29261 [==============================] - 41s - loss: 8.4871 - acc: 0.6298 - val_loss: 10.0028 - val_acc: 0.5766\n",
      "Epoch 7/20\n",
      "29261/29261 [==============================] - 41s - loss: 7.5504 - acc: 0.6601 - val_loss: 9.4108 - val_acc: 0.6067\n",
      "Epoch 8/20\n",
      "29261/29261 [==============================] - 41s - loss: 6.9064 - acc: 0.6841 - val_loss: 9.9388 - val_acc: 0.6113\n",
      "Epoch 9/20\n",
      "29261/29261 [==============================] - 41s - loss: 6.3472 - acc: 0.7046 - val_loss: 9.2711 - val_acc: 0.6298\n",
      "Epoch 10/20\n",
      "29261/29261 [==============================] - 41s - loss: 5.4725 - acc: 0.7337 - val_loss: 9.5911 - val_acc: 0.6310\n",
      "Epoch 11/20\n",
      "29261/29261 [==============================] - 41s - loss: 4.8266 - acc: 0.7574 - val_loss: 10.2665 - val_acc: 0.6313\n",
      "Epoch 12/20\n",
      "29261/29261 [==============================] - 41s - loss: 4.4389 - acc: 0.7719 - val_loss: 10.2618 - val_acc: 0.6479\n",
      "Epoch 13/20\n",
      "29261/29261 [==============================] - 41s - loss: 3.9126 - acc: 0.7911 - val_loss: 11.1226 - val_acc: 0.6436\n",
      "Epoch 14/20\n",
      "29261/29261 [==============================] - 41s - loss: 3.6765 - acc: 0.8027 - val_loss: 11.8496 - val_acc: 0.6451\n",
      "Epoch 15/20\n",
      "29261/29261 [==============================] - 41s - loss: 3.1593 - acc: 0.8229 - val_loss: 12.1363 - val_acc: 0.6451\n",
      "Epoch 16/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.9525 - acc: 0.8333 - val_loss: 12.4862 - val_acc: 0.6673\n",
      "Epoch 17/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.7700 - acc: 0.8450 - val_loss: 13.6832 - val_acc: 0.6531\n",
      "Epoch 18/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.6142 - acc: 0.8505 - val_loss: 13.4132 - val_acc: 0.6661\n",
      "Epoch 19/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.5989 - acc: 0.8561 - val_loss: 12.5584 - val_acc: 0.6725\n",
      "Epoch 20/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.3064 - acc: 0.8675 - val_loss: 12.7656 - val_acc: 0.6697\n"
     ]
    }
   ],
   "source": [
    "history = fit(model, xtrain, ytrain,epochs=20,class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29261 samples, validate on 3252 samples\n",
      "Epoch 1/20\n",
      "29261/29261 [==============================] - 41s - loss: 3.0291 - acc: 0.8468 - val_loss: 12.3531 - val_acc: 0.6731\n",
      "Epoch 2/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.6095 - acc: 0.8591 - val_loss: 12.7602 - val_acc: 0.6802\n",
      "Epoch 3/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.1384 - acc: 0.8789 - val_loss: 13.2103 - val_acc: 0.6900\n",
      "Epoch 4/20\n",
      "29261/29261 [==============================] - 41s - loss: 2.1598 - acc: 0.8800 - val_loss: 14.7414 - val_acc: 0.6645\n",
      "Epoch 5/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.8317 - acc: 0.8910 - val_loss: 13.2424 - val_acc: 0.6780\n",
      "Epoch 6/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.6018 - acc: 0.9058 - val_loss: 14.4386 - val_acc: 0.6836\n",
      "Epoch 7/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.3764 - acc: 0.9128 - val_loss: 15.9088 - val_acc: 0.6860\n",
      "Epoch 8/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.5473 - acc: 0.9056 - val_loss: 15.6779 - val_acc: 0.6796\n",
      "Epoch 9/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.4764 - acc: 0.9098 - val_loss: 14.0118 - val_acc: 0.6962\n",
      "Epoch 10/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.4161 - acc: 0.9154 - val_loss: 14.5603 - val_acc: 0.6820\n",
      "Epoch 11/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.2325 - acc: 0.9231 - val_loss: 17.4584 - val_acc: 0.6725\n",
      "Epoch 12/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.1226 - acc: 0.9291 - val_loss: 16.3048 - val_acc: 0.6830\n",
      "Epoch 13/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.3084 - acc: 0.9240 - val_loss: 14.8787 - val_acc: 0.6839\n",
      "Epoch 14/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.3009 - acc: 0.9257 - val_loss: 16.1251 - val_acc: 0.6885\n",
      "Epoch 15/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.1516 - acc: 0.9305 - val_loss: 15.8569 - val_acc: 0.6851\n",
      "Epoch 16/20\n",
      "29261/29261 [==============================] - 41s - loss: 0.9350 - acc: 0.9392 - val_loss: 17.3126 - val_acc: 0.6885\n",
      "Epoch 17/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.0679 - acc: 0.9329 - val_loss: 16.2924 - val_acc: 0.6931\n",
      "Epoch 18/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.1696 - acc: 0.9324 - val_loss: 15.9051 - val_acc: 0.6817\n",
      "Epoch 19/20\n",
      "29261/29261 [==============================] - 41s - loss: 1.1590 - acc: 0.9307 - val_loss: 15.3085 - val_acc: 0.6946\n",
      "Epoch 20/20\n",
      "29261/29261 [==============================] - 41s - loss: 0.8434 - acc: 0.9463 - val_loss: 16.6695 - val_acc: 0.6974\n"
     ]
    }
   ],
   "source": [
    "history = fit(model, xtrain, ytrain,epochs=20,class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_ltsm_data(raw_text):\n",
    "    chars = sorted(list(set(raw_text)))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(raw_text)\n",
    "    n_vocab = len(chars)\n",
    "    print \"Total Characters: \", n_chars\n",
    "    print \"Total Vocab: \", n_vocab\n",
    "    # prepare the dataset of input to output pairs encoded as integers\n",
    "    seq_length = 100\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = raw_text[i:i + seq_length]\n",
    "        seq_out = raw_text[i + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    # normalize\n",
    "    X = X / float(n_vocab)\n",
    "    # one hot encode the output variable\n",
    "    y = np_utils.to_categorical(dataY)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ltsm_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 0.0,\n",
       " 2: 0.0,\n",
       " 3: 0.0,\n",
       " 4: 0.0,\n",
       " 5: 0.0,\n",
       " 6: 0.0,\n",
       " 7: 0.0,\n",
       " 8: 0.0,\n",
       " 9: 0.0,\n",
       " 10: 0.0,\n",
       " 11: 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_table('xtrain_obfuscated.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lab = pd.read_table('ytrain.txt',header=None,delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[1] = train_lab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "shuffled_train = train_df.iloc[np.random.permutation(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_data = shuffled_train.iloc[:3200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data.columns = ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_train_data = shuffled_train.iloc[3200:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_train_data = sampled_train_data.iloc[np.random.permutation(len(sampled_train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_train_data.columns= ['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_counts = sampled_train_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_unique_labels = sampled_train_data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_resample(labels,label_counts,df):\n",
    "    max_count = np.max(label_counts)\n",
    "    df.sort_values('label',inplace=True)\n",
    "    new_df = df\n",
    "    for label in labels:\n",
    "        label_df = df.loc[df['label']==label]\n",
    "        diff = max_count-label_counts.iloc[label]\n",
    "        to_add = float(diff)/float(label_counts.loc[label])\n",
    "        fraction_samples = to_add if to_add<1 else to_add-round(to_add)\n",
    "        for i in range(0,int(round(to_add))):\n",
    "            new_df = new_df.append(label_df)\n",
    "        if fraction_samples >0:\n",
    "            samples = int(round(label_df.shape[0]*fraction_samples))\n",
    "            frac_df = label_df.iloc[:samples,:]\n",
    "            new_df = new_df.append(frac_df)\n",
    "    return new_df.iloc[np.random.permutation(len(new_df))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resampled_data = build_resample(sample_unique_labels,label_counts,sampled_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72701, 2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding Val data back to training set\n",
    "resampled_data = resampled_data.append(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75901, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     5097\n",
       "6     4226\n",
       "3     4023\n",
       "8     3634\n",
       "1     3459\n",
       "10    3052\n",
       "4     2337\n",
       "5     2283\n",
       "2     1471\n",
       "11    1408\n",
       "9      980\n",
       "0      543\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain, ytrain, xtest, vocab, max_len, n_classes = preprocess(\n",
    "        resampled_data['text'],\n",
    "        resampled_data['label'],\n",
    "        lines('xtest_obfuscated.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = compiled(char_cnn(len(vocab), max_len, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68310 samples, validate on 7591 samples\n",
      "Epoch 1/3\n",
      "68310/68310 [==============================] - 96s - loss: 2.3566 - acc: 0.1588 - val_loss: 2.3756 - val_acc: 0.1580\n",
      "Epoch 2/3\n",
      "68310/68310 [==============================] - 96s - loss: 2.3493 - acc: 0.1615 - val_loss: 2.3737 - val_acc: 0.1580\n",
      "Epoch 3/3\n",
      "68310/68310 [==============================] - 96s - loss: 2.3483 - acc: 0.1617 - val_loss: 2.3775 - val_acc: 0.1580\n"
     ]
    }
   ],
   "source": [
    "history = fit(model, xtrain, ytrain,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68310 samples, validate on 7591 samples\n",
      "Epoch 1/3\n",
      "68310/68310 [==============================] - 265s - loss: 2.0310 - acc: 0.2832 - val_loss: 1.4955 - val_acc: 0.4654\n",
      "Epoch 2/3\n",
      "68310/68310 [==============================] - 264s - loss: 1.2918 - acc: 0.5372 - val_loss: 1.2271 - val_acc: 0.5647\n",
      "Epoch 3/3\n",
      "68310/68310 [==============================] - 264s - loss: 0.9458 - acc: 0.6651 - val_loss: 0.8877 - val_acc: 0.6841\n"
     ]
    }
   ],
   "source": [
    "history = fit(model, xtrain, ytrain,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68310 samples, validate on 7591 samples\n",
      "Epoch 1/20\n",
      "68310/68310 [==============================] - 264s - loss: 0.6828 - acc: 0.7692 - val_loss: 0.6181 - val_acc: 0.7892\n",
      "Epoch 2/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.4814 - acc: 0.8409 - val_loss: 0.5021 - val_acc: 0.8369\n",
      "Epoch 3/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.3623 - acc: 0.8812 - val_loss: 0.4780 - val_acc: 0.8442\n",
      "Epoch 4/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.2819 - acc: 0.9084 - val_loss: 0.3482 - val_acc: 0.8920\n",
      "Epoch 5/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.2249 - acc: 0.9282 - val_loss: 0.2833 - val_acc: 0.9170\n",
      "Epoch 6/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1923 - acc: 0.9383 - val_loss: 0.2671 - val_acc: 0.9194\n",
      "Epoch 7/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1788 - acc: 0.9439 - val_loss: 0.2810 - val_acc: 0.9215\n",
      "Epoch 8/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1489 - acc: 0.9530 - val_loss: 0.2276 - val_acc: 0.9398\n",
      "Epoch 9/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1313 - acc: 0.9592 - val_loss: 0.2584 - val_acc: 0.9351\n",
      "Epoch 10/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1183 - acc: 0.9641 - val_loss: 0.2631 - val_acc: 0.9308\n",
      "Epoch 11/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1142 - acc: 0.9649 - val_loss: 0.2285 - val_acc: 0.9430\n",
      "Epoch 12/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.1126 - acc: 0.9651 - val_loss: 0.2187 - val_acc: 0.9480\n",
      "Epoch 13/20\n",
      "68310/68310 [==============================] - 262s - loss: 0.0982 - acc: 0.9705 - val_loss: 0.2819 - val_acc: 0.9286\n",
      "Epoch 14/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.0897 - acc: 0.9736 - val_loss: 0.1978 - val_acc: 0.9499\n",
      "Epoch 15/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.0831 - acc: 0.9740 - val_loss: 0.1823 - val_acc: 0.9569\n",
      "Epoch 16/20\n",
      "68310/68310 [==============================] - 263s - loss: 0.0877 - acc: 0.9743 - val_loss: 0.2434 - val_acc: 0.9452\n",
      "Epoch 17/20\n",
      "68310/68310 [==============================] - 262s - loss: 0.0826 - acc: 0.9750 - val_loss: 0.2112 - val_acc: 0.9472\n",
      "Epoch 18/20\n",
      "68310/68310 [==============================] - 262s - loss: 0.0886 - acc: 0.9740 - val_loss: 0.2121 - val_acc: 0.9468\n",
      "Epoch 19/20\n",
      "68310/68310 [==============================] - 262s - loss: 0.0821 - acc: 0.9760 - val_loss: 0.1934 - val_acc: 0.9577\n",
      "Epoch 20/20\n",
      "68310/68310 [==============================] - 262s - loss: 0.0748 - acc: 0.9781 - val_loss: 0.2566 - val_acc: 0.9459\n"
     ]
    }
   ],
   "source": [
    "history = fit(model, xtrain, ytrain,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('weight_oversampling1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
